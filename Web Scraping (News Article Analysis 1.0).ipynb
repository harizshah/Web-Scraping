{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "355159e5",
   "metadata": {},
   "source": [
    "# News Article Analysis 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d75970",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f83e5d",
   "metadata": {},
   "source": [
    "### Keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee5043",
   "metadata": {},
   "source": [
    "Specify a 'keyword' to search for in Google News. Tool created for this example only design to scrap articles from www.thestar.com.my"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d308c8",
   "metadata": {},
   "source": [
    "Search in Google News will be looks like this: \"Petronas\" site:www.thestar.com.my"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc066b59",
   "metadata": {},
   "source": [
    "### Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d99905",
   "metadata": {},
   "source": [
    "Specify pages of result from Google News to scrap. Generally one page of Google News result contains 10 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12247644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEYWORD to search in Google News\n",
    "keyword = 'Harimau Malaya'\n",
    "\n",
    "# PAGES to download from Google News\n",
    "pages = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e820e",
   "metadata": {},
   "source": [
    "### Store of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3815d",
   "metadata": {},
   "source": [
    "Result will be save in a all_articles List, that contain List."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a50c7",
   "metadata": {},
   "source": [
    "all_articles = [[date, title, content, link],[date, ..., ..., ...],....,....]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ff454",
   "metadata": {},
   "source": [
    "### Adjust keyword casing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba22960",
   "metadata": {},
   "source": [
    "Adjust keyword to lower case, standardize casing for text matching and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88655feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All articles with date, title, content, link will be save in list of list\n",
    "all_articles = []\n",
    "\n",
    "# turn keyword into lower case\n",
    "keyword = keyword.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bca06",
   "metadata": {},
   "source": [
    "## Import module needed for Web ScrapingÂ¶ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c5c23",
   "metadata": {},
   "source": [
    "requests - use for downloading html code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988992a",
   "metadata": {},
   "source": [
    "BeautifulSoup - use for parsing html code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760dc90",
   "metadata": {},
   "source": [
    "Regular Expressiong (re) - use for search and matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc693db",
   "metadata": {},
   "source": [
    "time - using its time.sleep() function to slower the scraping (hopefully less burder to target website server)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072daac1",
   "metadata": {},
   "source": [
    "random - to make sleep time in random seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f451fd",
   "metadata": {},
   "source": [
    "datetime - to re-format date downloaded from articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46fbe98",
   "metadata": {},
   "source": [
    "unicodedata - to clean up some unicode in article's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be661c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import unicodedata  # to clean unicode eg. \\xa0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9374b484",
   "metadata": {},
   "source": [
    "## Scrap Individual Article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dfd46f",
   "metadata": {},
   "source": [
    "Define scrapTheStar(link) - define function to scrap single page of The Star Online"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22407a4",
   "metadata": {},
   "source": [
    "Date, Title, Content, Link of the article will be added to all_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d8d0c",
   "metadata": {},
   "source": [
    "If intended to scrap news article from other site, this function will need to be re-write. As different website will have different html structure, storing data in different place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to scrap The Star Online (date, title, content, link)\n",
    "def scrapTheStar(link):\n",
    "    page_response = requests.get(link, timeout=5)\n",
    "    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "    \n",
    "    # scrap date\n",
    "    date = page_content.select('p.date')\n",
    "    date = date[0].text.strip()     # Tuesday, 9 Oct 2018\n",
    "    date = re.findall(r'\\w+', date) \n",
    "    date = ' '.join(date[1:4])      # 9 Oct 2018\n",
    "    date = datetime.datetime.strptime(date,'%d %b %Y').strftime('%Y-%m-%d')  # 20181009\n",
    "    \n",
    "    # scrap title\n",
    "    titles = page_content.select('h1')\n",
    "    title = titles[0].text.strip()\n",
    "    \n",
    "    # scrap content\n",
    "    nodes = page_content.select('div.story p')\n",
    "    content = ''\n",
    "    for node in nodes:\n",
    "        content = content + node.text\n",
    "    content = unicodedata.normalize(\"NFKD\",content)\n",
    "    \n",
    "    # scrap content (alternative method, if above method failed)\n",
    "    if len(content) < 10:\n",
    "        try:\n",
    "            content = page_content.select('div.story')\n",
    "            content = content[1].text\n",
    "        except IndexError:\n",
    "            print('unable to download content')\n",
    "            \n",
    "    # gathering information into a list\n",
    "    date_title_content = [date, title, content, link]\n",
    "    \n",
    "    # Note: Google result may have article which keyword not exist in content (only exist in related news title ).\n",
    "    # only append those articles with keyword in content\n",
    "    if keyword in date_title_content[2].lower():  \n",
    "        all_articles.append(date_title_content)\n",
    "    else: \n",
    "        print('keyword not in content')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c6ed77",
   "metadata": {},
   "source": [
    "## Scrap URL from Google News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d8f4a",
   "metadata": {},
   "source": [
    "Define scrapit(googleNewsUrl) - define function to scrap news articles URL from Google News results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b10be",
   "metadata": {},
   "source": [
    "When running this function, scrapTheStar(link) function will be call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f95362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to scrap Google News, loop through all pages to get The Star Online url.\n",
    "def scrapit(googleNewsUrl):\n",
    "    res = requests.get(googleNewsUrl, timeout=5)\n",
    "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "    \n",
    "    links = soup.select('h3 a')\n",
    "\n",
    "    for link in links:\n",
    "        link = link.get('href')\n",
    "        urlRegex = re.compile('https://www.thestar.com.my.*/&sa')  # define match https..../&sa\n",
    "        link = urlRegex.findall(link)  # find match in link\n",
    "        link = link[0][:-4]   # regex return a list so call index [0], [:-4]strip /&sa which only need for match\n",
    "        #print(link)\n",
    "        scrapTheStar(link)\n",
    "        \n",
    "        # make random sleep to slow down the scraping\n",
    "        r = random.randint(1, 5)\n",
    "        #print('sleep', r, 'seconds')\n",
    "        time.sleep(r)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe7531",
   "metadata": {},
   "source": [
    "## Web Scraping in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf29919",
   "metadata": {},
   "source": [
    "Inserting keyword to googleNewsUrl, looping through number of page specified, and calling the scrapit(googleNewsUrl) function, which will also call scrapTheStar(link) inside of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca579e",
   "metadata": {},
   "source": [
    "Optional: print out URL of Google News and individual articles for checking. Optional: print out sleep in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5dc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(pages):\n",
    "    keyword_in_link = '+'.join(keyword.split())  # add + between keyword\n",
    "    googleNewsUrl = 'https://www.google.com/search?q=%22' + keyword_in_link +'%22+site:www.thestar.com.my&hl=en&tbm=nws&ei=3u2wW7rtLMmKvQTTo77QCw&start=' + str(page) + '0&sa=N'\n",
    "    #print(googleNewsUrl)\n",
    "    scrapit(googleNewsUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5bcf9",
   "metadata": {},
   "source": [
    "## Save all_articles as JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172aa0c0",
   "metadata": {},
   "source": [
    "After saving a copy in hard drive, we can use it for Text Analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f83660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "stringOfJsonData = json.dumps(all_articles)\n",
    "jsonFile = open('news.json', 'w')\n",
    "jsonFile.write(stringOfJsonData)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae34958",
   "metadata": {},
   "source": [
    "## Result of Web Scraping tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04536cbd",
   "metadata": {},
   "source": [
    "Open up news.json to check Web Scraping result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d22b3",
   "metadata": {},
   "source": [
    "Example: print out all title from result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe6f7f",
   "metadata": {},
   "source": [
    "Results store in : all_articles = [[date, title, content, link],[date, ..., ..., ...],....,....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9a0e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open JSON file\n",
    "with open('news.json') as f:\n",
    "    all_articles = json.load(f)\n",
    "\n",
    "# Print out all title, with index in front\n",
    "index = 1\n",
    "for i in range(len(all_articles)):\n",
    "    print(str(index) + ') ' + all_articles[i][1])\n",
    "    index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
